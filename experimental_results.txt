Experiment: 06/30/2022, 00:25:12
model Autoencoder
model detail: Autoencoder(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss -1480.11024375
epochs 1
time 1.8160008509953818
-------------------------
Experiment: 06/30/2022, 00:28:53
model Autoencoder
model detail: Autoencoder(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss function SSIM
epochs 1
loss -1621.8762829296875
time 1.849831998348236
-------------------------
Experiment: 06/30/2022, 00:28:53
model Autoencoder
model detail: Autoencoder(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss: function SSIM
epochs: 1
loss: -1621.8762829296875
time: 1.849831998348236
-------------------------
Experiment: 06/30/2022, 19:03:41
model Autoencoder_ConvTranspose
model detail: Autoencoder_ConvTranspose(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss function SSIM
epochs: 10
loss: 0.20177
time: 14.04
-------------------------
Experiment: 07/01/2022, 14:41:12
model Autoencoder_ConvTranspose
model detail: Autoencoder_ConvTranspose(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(300, 200, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(200, 100, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(100, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    )
  )
)
loss function MSE
epochs: 10
loss: 0.00264
time: 7.75
-------------------------
Experiment: 07/02/2022, 14:11:19
model Autoencoder_Upsampling
model detail: Autoencoder_Upsampling(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): Conv2d(300, 200, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=bilinear)
      (2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(200, 100, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=bilinear)
      (2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(100, 3, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=2.0, mode=bilinear)
    )
  )
)
loss function MSE
epochs: 1
losses: [0.0028511129170656203]
time: 2.08
-------------------------
Experiment: 07/02/2022, 14:13:43
model Autoencoder_Upsampling
model detail: Autoencoder_Upsampling(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): Conv2d(300, 200, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=bilinear)
      (2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(200, 100, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=bilinear)
      (2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(100, 3, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=2.0, mode=bilinear)
    )
  )
)
loss function MSE
epochs: 1
losses: [0.0028444305628538133]
time: 1.09
-------------------------
Experiment: 07/02/2022, 14:21:53
model Autoencoder_ConvTranspose
model detail: Autoencoder_ConvTranspose(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(300, 200, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(200, 100, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(100, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    )
  )
)
loss function MSE
epochs: 1
losses: [0.002867938536405563]
time: 1.24
-------------------------
Experiment: 07/02/2022, 14:29:54
model Autoencoder_ConvTranspose
model detail: Autoencoder_ConvTranspose(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(300, 200, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(200, 100, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2), output_padding=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(100, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))
    )
  )
)
loss function MSE
epochs: 1
losses: [0.002882401409745216]
time: 1.22
-------------------------
Experiment: 07/02/2022, 14:48:08
model Autoencoder
model detail: Autoencoder(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss function MSE
epochs: 7
losses: [0.0005382676159963012, 0.00027495564743876456, 0.00021381862768903376, 0.00018433325607329607, 0.00015252209939062595, 0.00014154349593445658, 0.00014929207833483816]
time: 6.69
Comments: Check how it works with initial model Autoencoder
-------------------------
Experiment: 07/02/2022, 15:02:15
model Autoencoder
model detail: Autoencoder(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(60, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(120, 240, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=3840, out_features=300, bias=True)
  (rev_linear): Linear(in_features=300, out_features=3840, bias=True)
  (decoder): Sequential(
    (0): Sequential(
      (0): ConvTranspose2d(240, 120, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): ConvTranspose2d(120, 60, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): ConvTranspose2d(60, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)
loss function MSE
epochs: 15
losses: [0.03756, 0.01871, 0.01393, 0.01149, 0.00985, 0.009, 0.00866, 0.0085, 0.00869, 0.00812, 0.00807, 0.0081, 0.00806, 0.0077, 0.00785]
time: 14.65
Comments: Changed test_loss: now it's calculating by devision by len(test_loader) not len(test_loader.dataset)
-------------------------
Experiment: 07/02/2022, 18:43:54
model Autoencoder_Upsampling
model detail: Autoencoder_Upsampling(
  (encoder): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(200, 300, kernel_size=(2, 2), stride=(1, 1))
    )
  )
  (decoder): Sequential(
    (0): Sequential(
      (0): Conv2d(300, 200, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=nearest)
      (2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (1): Sequential(
      (0): Conv2d(200, 100, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=4.0, mode=nearest)
      (2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): Dropout(p=0.2, inplace=False)
      (4): LeakyReLU(negative_slope=0.2)
    )
    (2): Sequential(
      (0): Conv2d(100, 3, kernel_size=(1, 1), stride=(1, 1))
      (1): Upsample(scale_factor=2.0, mode=nearest)
    )
  )
)
loss function MSE
Comments: 
epochs: 2
losses: [0.18099]
time: 1.09
-------------------------
